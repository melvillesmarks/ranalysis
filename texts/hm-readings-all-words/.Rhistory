x <- 0:10  # Assigns number 0 through 10 to x
x  # Prints contents of x in console
y <- c(5, 4, 1, 6, 7, 2, 2, 3, 2, 8)  # Assigns values to y
y  # Prints y to console
ls()  # List objects
sn.csv <- read.csv("C:\Users\Christopher M. Ohge\Desktop\social_network.csv", header = T)
sn.csv <- read.csv("C:/Users\Christopher M. Ohge\Desktop\social_network.csv", header = T)
sn.csv <- read.csv("christopherohge/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/christopherohge/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/christopherohge/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/christopherohge/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/Desktop/social_network.csv", header = T)
sn.csv <- read.csv("~/Desktop/social_network.csv", header = T)
str(sn.csv)
sn.spss.csv <- read.csv("~/Desktop/social_network_spss.csv", header = T)
browseURL("http://cran.r-project.org/web/views/")
browseURL("http://cran.stat.ucla.edu/web/packages/available_packages_by_name.html")
browseURL("http://cran.r-project.org/web/views/")
browseURL("http://cran.stat.ucla.edu/web/packages/available_packages_by_name.html")
library()  # Brings up editor list of available packages
install.packages("psych")
library("stylo", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
sn <- read.csv("social_network.csv", header = T)
sn <- read.csv("~/Desktop/social_network.csv", header = T)
site.freq <- table(sn$Site)  # Creates table from Site
barplot(site.freq)  # Creates barplot in new window
? barplot  # For more information on customizing graph
barplot(site.freq[order(site.freq, decreasing = T)])
barplot(site.freq[order(site.freq)], horiz = T)
barplot(site.freq[order(site.freq, decreasing = T)])
barplot(site.freq[order(site.freq)], horiz = T)
fbba <- c(rep("gray", 5),
rgb(59, 89, 152, maxColorValue = 255))
barplot(site.freq[order(site.freq)],
horiz = T,
col = fbba)
barplot(site.freq[order(site.freq)],
horiz = T,         # Horizontal
col = fbba,        # Use colors "fbba"
border = NA,       # No borders
xlim = c(0, 100),  # Scale from 0-100
main = "Preferred Social Networking Site\nA Survey of 202 Users",
xlab = "Number of Users")
library("XML", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("qdap")
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("qdap", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("rJava")
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
detach("package:rJava", unload=TRUE)
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
setwd("~/GitSpace/ranalysis/markup/")
library(XML)
doc <- xmlTreeParse("460-markings-only.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@play='1a']")
divs.ns.l
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
div.raws.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
(divs.genres.l)
genres.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
genres.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- genres.hapax.v / genres.lengths.m
barplot(hapax.percentage, main = "Hapax analysis in The Tempest",
ylab = "lexical uniqueness",
xlab = "Number of markings",
beside=T, col="grey")
axis(1, at = seq(1, 17,by=1), labels = FALSE, tck=-.015)
xtick <- seq(1, 16, by=1)
text(x=xtick,  par("usr")[3], cex=.75, pos=1,
labels = xtick, srt = 0, xpd = TRUE)
doc <- xmlTreeParse("358b-stripped.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@play='1a']")
divs.ns.l <- getNodeSet(doc, "/body//*[@section='pl8']")
divs.ns.l
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
div.raws.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
genres.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
genres.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- genres.hapax.v / genres.lengths.m
barplot(hapax.percentage, main = "Hapax analysis in Book 8 of Paradise Lost",
ylab = "lexical uniqueness",
xlab = "Number of markings",
beside=T, col="grey")
barplot(hapax.percentage, main = "Hapax analysis in Book 8 of Paradise Lost",
ylab = "lexical uniqueness",
xlab = "Number of markings",
beside=T, col="grey")
axis(1, at = seq(1, 22,by=1), labels = FALSE, tck=-.015)
xtick <- seq(1, 21, by=1)
text(x=xtick,  par("usr")[3], cex=.75, pos=1,
labels = xtick, srt = 0, xpd = TRUE)
words <- data_frame(file = paste0("hm-readings-all-words/",
c("hm-markings-milton.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
allwords <- data_frame(file = paste0("~hm-readings-all-words//",
c("hm-marked-words-all.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
words <- data_frame(file = paste0("hm-readings-all-words/",
c("hm-markings-milton.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
words <- data_frame(file = paste0("/hm-readings-all-words/",
c("hm-markings-milton.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
setwd("~/GitSpace/ranalysis/texts/hm-readings-all-words/")
library(XML)
library(tidytext)
library(dplyr)
library(stringr)
library(glue)
library(tidyverse)
words <- data_frame(file = paste0(c("hm-markings-milton.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
words_sentiment <- inner_join(words,
get_sentiments("bing")) %>%
count(file, index = round(line_number/ max(line_number) * 100 / 5) * 5, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(net_sentiment = positive - negative)
words_sentiment %>% ggplot(aes(x = index, y = net_sentiment, fill = file)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ file) +
scale_x_continuous("Location in the volume") +
scale_y_continuous("Bing net Sentiment")
bing_word_counts <- words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Sentiment Frequencies in Melville's Markings in Homer, Milton, and Shakespeare",
x = NULL) +
coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Sentiment Frequencies in Melville's Markings in Milton",
x = NULL) +
coord_flip()
setwd("~/GitSpace/ranalysis/texts/hm-readings-all-words/")
library(XML)
library(tidytext)
library(dplyr)
library(stringr)
library(glue)
library(tidyverse)
words <- data_frame(file = paste0(c("hm-markings-milton.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
words_sentiment <- inner_join(words,
get_sentiments("bing")) %>%
count(file, index = round(line_number/ max(line_number) * 100 / 5) * 5, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(net_sentiment = positive - negative)
words_sentiment %>% ggplot(aes(x = index, y = net_sentiment, fill = file)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ file) +
scale_x_continuous("Location in the volume") +
scale_y_continuous("Bing net Sentiment")
bing_word_counts <- words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Sentiment Frequencies in Melville's Markings in Milton",
x = NULL) +
coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Sentiment Frequencies of Marked Passages in Milton",
x = NULL) +
coord_flip()
setwd("~/GitSpace/ranalysis/markup/")
library(XML)
doc <- xmlTreeParse("460-markings-only.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@play='1a']")
divs.ns.l
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
div.raws.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
genres.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
genres.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- genres.hapax.v / genres.lengths.m
barplot(hapax.percentage, main = "Hapax Analysis of Marked Passages in The Tempest",
ylab = "Lexical uniqueness",
xlab = "Marked passages",
beside=T, col="grey")
axis(1, at = seq(1, 17,by=1), labels = FALSE, tck=-.015)
xtick <- seq(1, 16, by=1)
text(x=xtick,  par("usr")[3], cex=.75, pos=1,
labels = xtick, srt = 0, xpd = TRUE)
doc <- xmlTreeParse("358b-stripped.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@section='pl8']")
divs.ns.l
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
div.raws.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
(divs.genres.l)
divs.genres.l
div.raws.l
genres.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
genres.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- genres.hapax.v / genres.lengths.m
barplot(hapax.percentage, main = "Hapax Analysis of Marked Passages in Book 8 of Paradise Lost",
ylab = "Lexical uniqueness",
xlab = "Marked passages",
beside=T, col="grey")
axis(1, at = seq(1, 22,by=1), labels = FALSE, tck=-.015)
xtick <- seq(1, 21, by=1)
text(x=xtick,  par("usr")[3], cex=.75, pos=1,
labels = xtick, srt = 0, xpd = TRUE)
words <- data_frame(file = paste0(c("hm-markings-all.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
setwd("~/GitSpace/ranalysis/texts/hm-readings-all-words/")
library(XML)
library(tidytext)
library(dplyr)
library(stringr)
library(glue)
library(tidyverse)
words <- data_frame(file = paste0(c("hm-markings-all.txt"))) %>%
mutate(text = map(file, read_lines)) %>%
unnest() %>%
group_by(file = str_sub(basename(file), 1, -5)) %>%
mutate(line_number = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text)
words_sentiment <- inner_join(words,
get_sentiments("bing")) %>%
count(file, index = round(line_number/ max(line_number) * 100 / 5) * 5, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(net_sentiment = positive - negative)
words_sentiment %>% ggplot(aes(x = index, y = net_sentiment, fill = file)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ file) +
scale_x_continuous("Location in the volume") +
scale_y_continuous("Bing net Sentiment")
bing_word_counts <- words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Sentiment Frequencies of Marked Passages in Milton",
x = NULL) +
coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Word Frequency of Marked Passages in Shakespeare, Milton, and Homer",
x = NULL) +
coord_flip()
