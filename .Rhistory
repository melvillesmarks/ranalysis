div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
div.words.v <- paste(sapply(words.ns, xmlValue), collapse=" ")
words.lower.v <- tolower(div.words.v)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
word.v <- unlist(words.l)
word.v <- word.v[which(word.v!="")]
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
div.freqs.un.v <- unlist(div.freqs.l)
sorted.div.freqs.v <- sort(div.freqs.un.v, decreasing=T)
write.table(sorted.div.freqs.v, "460.freqs.v.sorted.txt")
length(div.raws.l)
names(div.raws.l)
str(div.raws.l)
sum(div.raws.l[[1]])/length(div.raws.l[[1]])
mean(div.raws.l[[1]]) # meaning: each word type in the first chapter is used an average of 1.058 times
lapply(div.raws.l,mean)
mean.word.use.m <- do.call(rbind, lapply(div.raws.l,mean))
dim(mean.word.use.m)
plot(mean.word.use.m, type = "h", main = "Mean word usage patterns in Melville's markings in Shakespeare",
ylab = "mean word use", xlab = "node (each instance of marking)")
library(XML)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
div.words.v <- paste(sapply(words.ns, xmlValue), collapse=" ")
words.lower.v <- tolower(div.words.v)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
word.v <- unlist(words.l)
word.v <- word.v[which(word.v!="")]
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
div.freqs.un.v <- unlist(div.freqs.l)
sorted.div.freqs.v <- sort(div.freqs.un.v, decreasing=T)
write.table(sorted.div.freqs.v, "460.freqs.v.sorted.txt")
length(div.raws.l)
names(div.raws.l)
str(div.raws.l)
sum(div.raws.l[[1]])/length(div.raws.l[[1]])
mean(div.raws.l[[1]]) # meaning: each word type in the first chapter is used an average of 1.058 times
lapply(div.raws.l,mean)
mean.word.use.m <- do.call(rbind, lapply(div.raws.l,mean))
dim(mean.word.use.m)
plot(mean.word.use.m, type = "h", main = "Mean word usage patterns in Melville's markings in Shakespeare",
ylab = "mean word use", xlab = "node (each instance of marking)")
scale(mean.word.use.m)
plot(scale(mean.word.use.m), type = "h", main = "Scaled mean word usage patterns in Melville's markings in Shakespeare",
ylab = "mean word use", xlab = "node (each instance of marking):
1-298, comedies; 299-366, histories; 367-89, other; 390-682, tragedies")
lapply(div.raws.l,mean)
mean.word.use.m <- do.call(rbind, lapply(div.raws.l,mean))
dim(mean.word.use.m)
plot(mean.word.use.m, type = "h", main = "Mean word usage patterns in Melville's markings in Shakespeare",
ylab = "mean word use", xlab = "nodes (each marking):
1-298, comedies; 299-366, histories; 367-89, other; 390-682, tragedies")
scale(mean.word.use.m)
plot(scale(mean.word.use.m), type = "h", main = "Scaled mean word usage patterns in Melville's markings in Shakespeare",
ylab = "mean word use", xlab = "nodes (each marking):
1-298, comedies; 299-366, histories; 367-89, other; 390-682, tragedies")
mean.word.use.m[order(mean.word.use.m, decreasing=TRUE),]
length(div.raws.l[[1]])/sum(div.raws.l[[1]])*100
ttr.l <- lapply(div.raws.l, function(x) {length(x)/sum(x)*100})
ttr.m <- do.call(rbind, ttr.l)
ttr.m[order(ttr.m, decreasing = TRUE),]
plot(ttr.m, type = "h", main = "Type-token ratios in Melville's markings in Shakespeare",
ylab = "lexical variety", xlab = "nodes (each marking):
1-298, comedies; 299-366, histories; 367-89, other; 390-682, tragedies")
library("stylo", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
stylo(ttrl.l)
ttr.l <- lapply(div.raws.l, function(x) {length(x)/sum(x)*100})
ttr.m <- do.call(rbind, ttr.l)
stylo(ttrl.l)
stylo(div.freqs.l)
input.data <- read("460.xml")
my.corpus = load.corpus(files = c("460.xml") )
parse.corpus(input.data, markup.type = "xml", language = "English", splitting.rule = NULL, sample.size = 10000, sampling = "no.sampling", sample.overlap = 0, number.of.samples = 1, sampling.with.replacement = FALSE, features = "w",  ngram.size = 1, preserve.case = FALSE, encoding = "native.enc")
parse.corpus(my.corpus, markup.type = "xml", language = "English", splitting.rule = NULL, sample.size = 10000, sampling = "no.sampling", sample.overlap = 0, number.of.samples = 1, sampling.with.replacement = FALSE, features = "w",  ngram.size = 1, preserve.case = FALSE, encoding = "native.enc")
load.corpus.and.parse(files = "all", corpus.dir = "", markup.type = "xml",
language = "English", splitting.rule = NULL,
sample.size = 10000, sampling = "no.sampling",
sample.overlap = 0, number.of.samples = 1,
sampling.with.replacement = FALSE, features = "w",
ngram.size = 1, preserve.case = FALSE,
encoding = "native.enc")
my.corpus = load.corpus.and.parse(files = c("460.xml"),
markup.type = "xml", ngram.size = 2)
parse.corpus(my.corpus, markup.type = "xml", language = "English", splitting.rule = NULL, sample.size = 10000, sampling = "no.sampling", sample.overlap = 0, number.of.samples = 1, sampling.with.replacement = FALSE, features = "w",  ngram.size = 1, preserve.case = FALSE, encoding = "native.enc")
stylo(my.corpus)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
make.frequency.list(div.freqs.l)
make.frequency.list(div.freqs.l, value = FALSE, head = NULL,
relative = TRUE)
make.frequency.list(div.freqs.l, value = TRUE, head = NULL,
relative = TRUE)
make.frequency.list(ttr.m, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words(my.corpus)
make.frequency.list(words)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words)
make.frequency.list(words, value = TRUE)
make.ngrams(words, ngram.size = 1)
make.ngrams(words, ngram.size = 2)
make.ngrams(words, ngram.size = 1)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
make.table.of.frequencies(words, features, absent.sensitive = TRUE, relative = TRUE)
make.table.of.frequencies(words, absent.sensitive = TRUE, relative = TRUE)
make.table.of.frequencies(words, features = DEFAULT absent.sensitive = TRUE, relative = TRUE)
make.table.of.frequencies(words, features = default, absent.sensitive = TRUE, relative = TRUE)
make.table.of.frequencies(words, features = DEFAULT, absent.sensitive = TRUE, relative = TRUE)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
print(words)
words <- txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
print(words)
stylo(my.corpus)
stylo(words)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM']
[not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
delete.markup(divs.ns.l, markup.type = "xml")
delete.markup(doc, markup.type = "xml")
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 2)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 2)
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 1)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
plot(complete.word.list, type = "h")
plot(complete.word.list, type = "h",  main = "Type-token ratios in Melville's markings in Shakespeare",
ylab = "lexical variety", xlab = "nodes (each marking):
1-298, comedies; 299-366, histories; 367-89, other; 390-682, tragedies")
write.table(sorted.div.freqs.v, "460.checkmarks.freqs.v.sorted.txt", labels = FALSE)
write.table(words, "460.word.txt")
make.table.of.frequencies(words, complete.word.list, "460.words.txt")
write.table(complete.word.list, "460.words.txt")
stylo(frequencies = "460.words.txt")
stylo(frequencies = "460.words.txt")
stylo(frequencies = "460.words.txt")
txts = list("460.words.txt")
stylo(parsed.corpus = txts)
txts = c("460.words.txt")
stylo(frequencies = txts)
460 = c("460.words.txt")
txt460 = c("460.words.txt")
data(txt460)
txt460 = read("460.words.txt")
txt460 = read.table("460.words.txt")
data(txt460)
stylo(frequencies = txt460)
txt <- read.table("460.words.txt")
stylo(frequencies = txt460)
stylo(frequencies = txt)
stylo(txt)
stylo(gui = TRUE,
txt)
stylo(gui = TRUE,
"460.words.txt")
txt <- read.table("460.words.txt")
print(txt)
data(txt)
stylo(frequencies = "460.words.txt", analysis.type = "CA", write.png.file = "TRUE",
custom.graph.title = "HM's Shakespeare", gui = FALSE)
HM.txt <- read.table("melville.txt")
WS.txt <- read.table("460.words.txt")
HM.txt <- read.table("melville.txt")
setwd()
HM.txt <- read.table("melville.txt")
HM.txt <- c("melville.txt")
oppose(test.corpus = WS.txt, HM.txt)
oppose(primary.corpus = WS.txt, secondary.corpus = HM.txt)
oppose(primary.corpus = "460.words.txt", secondary.corpus = "melville.txt")
WS.txt <- read.table("460.words.txt")
HM.txt <- c("melville.txt")
WS.txt <- scan("460.words.txt")
HM.txt <- scan("melville.txt")
WS.txt <- scan("460.words.txt", what = "character", sep = "\n")
HM.txt <- scan("melville.txt", what = "character", sep = "\n")
stylo(frequencies = WS.txt)
WS.txt = scan("460.words.txt", what = "character", sep = "\n")
stylo(frequencies = WS.txt)
stylo(frequencies = HM.txt)
stylo(features = WS.txt)
corpus =
stylo(features = "WS.txt")
corpus =
stylo(features = "melville.txt")
corpus = c("/corpus")
stylo(features = corpus)
corpus = c("~/GitSpace/ranalysis/corpus/")
stylo(features = corpus)
stylo(features = my.corpus)
corpus = scan("~/GitSpace/ranalysis/corpus/")
stylo(features = corpus)
stylo(frequencies = corpus)
oppose(frequencies = corpus)
stylo(features = corpus)
stylo(features = corpus)
stylo(features = "melville.txt")
setwd("~/GitSpace/ranalysis/")
oppose()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
stylo()
setwd("~/Desktop/git-space/ranalysis/")
source("~/Desktop/Jockers-TextAnalysisWithR/code/mmo-shakespeare-functions.R")
library(XML)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][@mode='tragedy']
[not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
divs.tragedy.l <- list()
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
divs.tragedy.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
(divs.tragedy.l)
divs.tragedy.l
div.raws.l
div.freqs.l
div.freqs.t
tragedy.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
tragedy.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- tragedy.hapax.v / tragedy.lengths.m
barplot(hapax.percentage, beside=T,col="grey", names.arg = seq(1:length(div.freqs.l)))
source("~/Desktop/Jockers-TextAnalysisWithR/code/mmo-shakespeare-functions.R")
library(XML)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][@mode='comedy']
[not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
divs.comedy.l <- list()
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
divs.comedy.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
divs.comedy.l
div.raws.l
div.freqs.l
div.freqs.t
comedy.hapax.v <- sapply(div.raws.l, function(x) sum(x == 1))
comedy.lengths.m <- do.call(rbind, lapply(div.raws.l,sum))
hapax.percentage <- comedy.hapax.v / comedy.lengths.m
barplot(hapax.percentage, beside=T,col="grey", names.arg = seq(1:length(div.freqs.l)))
setwd("~/GitSpace/ranalysis/")
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 2)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
write.table(complete.word.list, "460.words.txt")
stylo()
stylo()
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 2)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
write.table(complete.word.list, "460.words.txt")
stylo()
setwd("~/GitSpace/ranalysis/")
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 2)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
write.table(complete.word.list, "460.words.txt")
stylo()
my.corpus = load.corpus.and.parse(files = c("460-markings-only.xml"),
markup.type = "xml", ngram.size = 1)
make.frequency.list(my.corpus, value = FALSE, head = NULL,
relative = TRUE)
words = txt.to.words.ext(my.corpus)
make.frequency.list(words, value = TRUE)
make.samples(words, sampling = "normal.sampling", sample.size = 50)
complete.word.list = make.frequency.list(words)
make.table.of.frequencies(words, complete.word.list)
write.table(complete.word.list, "460.words.txt")
stylo()
library(XML)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
divs.checkmark.underline.l <- list()
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
divs.checkmark.underline.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
divs.checkmark.underline.l
div.raws.l
div.freqs.l
love <- do.call(rbind, lapply(div.freqs.l, '[', 'love'))
death <- do.call(rbind, lapply(div.freqs.l, '[', 'death'))
print(div.raws.l)
length(div.raws.l)
length(div.raws.l[1])
summary(div.raws.l)
count(div.raws.l)
install.packages("plyr")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
count(div.raws.l)
count(div.raws.l)
x = count(div.freqs.l, c('nature', 'world'))
x
x = count(div.freqs.l, c('death', 'world'))
x = count(div.freqs.l)
x = count(word.v, c('nature', 'world''))
x
#isolates instances and node-level relative frequency for specified words
love <- do.call(rbind, lapply(div.freqs.l, '[', 'love'))
death <- do.call(rbind, lapply(div.freqs.l, '[', 'death'))
love.death <- cbind(love, death)
#identifies positions wherein specified word instances have N/A and excludes those by converting this value to 0.00000
love.death[which(is.na(love.death))] <- 0
#specifies column names for x axis in barplot
colnames(love.death) <- c("love", "death")
#produces barplot for relative frequencies across nodes, with additional specifications for x and y axis labeling
barplot(love.death, beside=T, col="grey", main="Frequencies for 'love' and 'death' in passages marked by Herman Melville in Shakespeare's Plays", ylab = "Term percentage value within text marked", xlab = "Comedies, Histories, and Tragedies combined")
#coerce instances to a dataframe for purpose of correlation
love.death.df <- as.data.frame(love.death)
cor(love.death)
#determine correlation of frequency for specified words
cor.test(love.death.df$love, love.death.df$death)
mycor <- cor(love.death[,"love"], love.death[,"death"])
mycor
x = count(word.v, c('nature', 'world'))
x = count(divs.ns.l, c('nature', 'world'))
a = table(div.raws.l)
a
words <- c(div.raws.l)
table(words)
y <- ldply(divs.ns.l[[1]], "rbind")
y <- ldply(words.v[[1]], "rbind")
library(XML)
doc <- xmlTreeParse("460.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
divs.checkmark.underline.l <- list()
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
divs.checkmark.underline.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
divs.checkmark.underline.l
y <- ldply(words.v[[1]], "rbind")
dim(y)
divs.checkmark.underline.l
y <- ldply(words.v[[1]], "rbind")
y <- ldply(word.v[[1]], "rbind")
dim(y)
y <- ldply(div.freqs.t[[1]], "rbind")
dim(y)
library("tm")
word.count(div.raws.l[1])
install.packages("tm")
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
word.count(div.raws.l[1])
count(div.raws.l[1])
dtm <- DocumentTermMatrix(doc)
dtm <- DocumentTermMatrix(divs.ns.l)
readXML(doc)
readXML(spec, doc)
readXML(type = "node", spec = "XPathExpression", doc)
readXML(spec = "XPathExpression", doc)
