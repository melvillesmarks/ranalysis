library(languageR)
inputDir <- "texts/"
files.v <- dir(inputDir, pattern = "460-markings-only.xml")
files.v
library(openNLP)
library(NLP)
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)
paras <- getNodeSet(doc.object,
"/body/div//w")
words <- as.String(paste(sapply(paras,xmlValue),
collapse=" ")) # Need sentence and word tokens first
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(words, list(sent_token_annotator,
word_token_annotator))
# now pos tags
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(words, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tagged_text <- paste(sprintf("%s/%s", words[a3w], tags),
collapse=" ")
write(tagged_text, paste("texts/taggedCorpus/",
files.v[i], ".txt", sep=""))
}
inputDir2 <- "texts/taggedCorpus/"
files.v2 <- dir(path = inputDir2, pattern = "460-markings-only.xml")
chunk.size <- 500
files.v2
splitText <- function(text) {
unlist(strsplit(text, " "))
}
selectTaggedWords <- function(tagged.words, target.tag) {
tagged.words[grep(target.tag, tagged.words)]
}
removeTags <- function(word.pos) {
sub("/[A-Z]{2,3}", "", word.pos)
}
makeFlexTextChunksFromTagged <- function(tagged.text,
chunk.size=500, percentage=TRUE){
tagged.words <- splitText(tagged.text)
tagged.words.keep <- c(selectTaggedWords(tagged.words, "/NN$"))
words <- removeTags(tagged.words.keep)
words.lower <- tolower(words)
word.v <- gsub("[^[:alnum:][:space:]']", "", words.lower)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]], chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
topic.m <- NULL
for(i in 1:length(files.v2)){
tagged.text <- scan(file.path(inputDir2, files.v2[i]),
what = "character", sep = "\n")
chunk.m <- makeFlexTextChunksFromTagged(tagged.text,
chunk.size, percentage = FALSE)
textname <- gsub("\\..*", "", files.v2[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep = "_"),
chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,
documents$text,
"texts/stoplist.csv",
FALSE,
token.regexp = "[\\p{L}']+")
topic.model <- MalletLDA(num.topics = 5)
class(topic.model)
topic.model$loadDocuments(mallet.instances)
vocab <- topic.model$getVocabulary()
vocab[1:50]
wordfreqs <- mallet.word.freqs(topic.model)
wordfreqs
topic.model$train(400)
topic.words.m <- mallet.topic.words(topic.model,
smoothed = TRUE,
normalized = TRUE)
dim(topic.words.m)
colnames(topic.words.m) <- vocab
library(wordcloud)
for(i in 1:5){
topic.top.words <- mallet.top.words(topic.model,
topic.words.m[i,], 100)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(1.8,.25), rot.per = 0,
random.order = F))
}
chunk.size <- 10
makeFlexTextChunks <- function(doc.object, chunk.size = 1000, percentage = TRUE)
if(length(chunks.l[[length(chunks.l)]]) <= chunk.size/2) {
chunks.l[[length(chunks.l)-1]] <- c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object, "/body/div//w")
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size)) #deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
textname <- gsub("\\..*","", files.v[i])
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size, percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname, segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,
documents$text,
"texts/stoplist.csv",
FALSE,
token.regexp = "[\\p{L}']+")
topic.model <- MalletLDA(num.topics = 5)
class(topic.model)
topic.model$loadDocuments(mallet.instances)
vocab <- topic.model$getVocabulary()
vocab[1:50]
wordfreqs <- mallet.word.freqs(topic.model)
wordfreqs
topic.model$train(400)
topic.words.m <- mallet.topic.words(topic.model,
smoothed = TRUE,
normalized = TRUE)
dim(topic.words.m)
colnames(topic.words.m) <- vocab
topic.words.m[1:5, 1:5]
wordcloud(topic.top.words$words,
topic.top.words$weights,
c(2.5,.7), rot.per=0, random.order=F)
doc.topics.m <- mallet.doc.topics(topic.model,
smoothed = T,
normalized = T)
file.ids.v <- documents[,1]
file.id.l <- strsplit(file.ids.v, "_")
file.chunk.id.l <- lapply(file.id.l, rbind)
file.chunk.id.m <- do.call(rbind, file.chunk.id.l)
head(file.chunk.id.m)
doc.topics.df <- as.data.frame(doc.topics.m)
doc.topics.df <- cbind(file.chunk.id.m[,1], doc.topics.df)
doc.topics.means.df <- aggregate(doc.topics.df[, 2:ncol(doc.topics.df)],
list(doc.topics.df[,1]),
mean)
library(openNLP)
library(NLP)
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)
paras <- getNodeSet(doc.object,
"/body/div//w")
words <- as.String(paste(sapply(paras,xmlValue),
collapse=" ")) # Need sentence and word tokens first
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(words, list(sent_token_annotator,
word_token_annotator))
# now pos tags
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(words, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tagged_text <- paste(sprintf("%s/%s", words[a3w], tags),
collapse=" ")
write(tagged_text, paste("texts/taggedCorpus/",
files.v[i], ".txt", sep=""))
}
inputDir2 <- "texts/taggedCorpus/"
files.v2 <- dir(path = inputDir2, pattern = "460-markings-only.xml")
chunk.size <- 500
files.v2
splitText <- function(text) {
unlist(strsplit(text, " "))
}
selectTaggedWords <- function(tagged.words, target.tag) {
tagged.words[grep(target.tag, tagged.words)]
}
removeTags <- function(word.pos) {
sub("/[A-Z]{2,3}", "", word.pos)
}
makeFlexTextChunksFromTagged <- function(tagged.text,
chunk.size=500, percentage=TRUE){
tagged.words <- splitText(tagged.text)
tagged.words.keep <- c(selectTaggedWords(tagged.words, "/NN$"))
words <- removeTags(tagged.words.keep)
words.lower <- tolower(words)
word.v <- gsub("[^[:alnum:][:space:]']", "", words.lower)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]], chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
topic.m <- NULL
for(i in 1:length(files.v2)){
tagged.text <- scan(file.path(inputDir2, files.v2[i]),
what = "character", sep = "\n")
chunk.m <- makeFlexTextChunksFromTagged(tagged.text,
chunk.size, percentage = FALSE)
textname <- gsub("\\..*", "", files.v2[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep = "_"),
chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,
documents$text,
"texts/stoplist.csv",
FALSE,
token.regexp = "[\\p{L}']+")
topic.model <- MalletLDA(num.topics = 5)
class(topic.model)
topic.model$loadDocuments(mallet.instances)
vocab <- topic.model$getVocabulary()
vocab[1:50]
wordfreqs <- mallet.word.freqs(topic.model)
wordfreqs
topic.model$train(400)
topic.words.m <- mallet.topic.words(topic.model,
smoothed = TRUE,
normalized = TRUE)
dim(topic.words.m)
colnames(topic.words.m) <- vocab
library(wordcloud)
for(i in 1:5){
topic.top.words <- mallet.top.words(topic.model,
topic.words.m[i,], 100)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(1.8,.25), rot.per = 0,
random.order = F))
}
makeFlexTextChunks <- function(doc.object, chunk.size = 1000, percentage = TRUE)
if(length(chunks.l[[length(chunks.l)]]) <= chunk.size/2) {
chunks.l[[length(chunks.l)-1]] <- c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
makeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){
paras <- getNodeSet(doc.object, "/body/div//w")
words <- paste(sapply(paras,xmlValue), collapse=" ")
words.lower <- tolower(words)
words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
words.l <- strsplit(words.lower, "\\s+")
word.v <- unlist(words.l)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size)) #deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
textname <- gsub("\\..*","", files.v[i])
topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size, percentage=FALSE)
textname <- gsub("\\..*","", files.v[i])
segments.m <- cbind(paste(textname, segment=1:nrow(chunk.m), sep="_"), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,
documents$text,
"texts/stoplist.csv",
FALSE,
token.regexp = "[\\p{L}']+")
topic.model <- MalletLDA(num.topics = 5)
class(topic.model)
topic.model$loadDocuments(mallet.instances)
vocab <- topic.model$getVocabulary()
vocab[1:50]
wordfreqs <- mallet.word.freqs(topic.model)
wordfreqs
topic.model$train(400)
topic.words.m <- mallet.topic.words(topic.model,
smoothed = TRUE,
normalized = TRUE)
wordcloud(topic.top.words$words,
topic.top.words$weights,
c(2,.5), rot.per=0, random.order=F)
wordcloud(topic.top.words$words,
topic.top.words$weights,
c(3,.5), rot.per=0, random.order=F)
wordcloud(topic.top.words$words,
topic.top.words$weights,
c(1.5,.5), rot.per=0, random.order=F)
doc.topics.m <- mallet.doc.topics(topic.model,
smoothed = T,
normalized = T)
file.ids.v <- documents[,1]
file.id.l <- strsplit(file.ids.v, "_")
file.chunk.id.l <- lapply(file.id.l, rbind)
file.chunk.id.m <- do.call(rbind, file.chunk.id.l)
head(file.chunk.id.m)
doc.topics.df <- as.data.frame(doc.topics.m)
doc.topics.df <- cbind(file.chunk.id.m[,1], doc.topics.df)
doc.topics.means.df <- aggregate(doc.topics.df[, 2:ncol(doc.topics.df)],
list(doc.topics.df[,1]),
mean)
library(openNLP)
library(NLP)
for(i in 1:length(files.v)){
doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)
paras <- getNodeSet(doc.object,
"/body/div//w")
words <- as.String(paste(sapply(paras,xmlValue),
collapse=" ")) # Need sentence and word tokens first
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(words, list(sent_token_annotator,
word_token_annotator))
# now pos tags
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(words, pos_tag_annotator, a2)
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tagged_text <- paste(sprintf("%s/%s", words[a3w], tags),
collapse=" ")
write(tagged_text, paste("texts/taggedCorpus/",
files.v[i], ".txt", sep=""))
}
inputDir2 <- "texts/taggedCorpus/"
files.v2 <- dir(path = inputDir2, pattern = "460-markings-only.xml")
chunk.size <- 500
files.v2
splitText <- function(text) {
unlist(strsplit(text, " "))
}
selectTaggedWords <- function(tagged.words, target.tag) {
tagged.words[grep(target.tag, tagged.words)]
}
removeTags <- function(word.pos) {
sub("/[A-Z]{2,3}", "", word.pos)
}
makeFlexTextChunksFromTagged <- function(tagged.text,
chunk.size=500, percentage=TRUE){
tagged.words <- splitText(tagged.text)
tagged.words.keep <- c(selectTaggedWords(tagged.words, "/NN$"))
words <- removeTags(tagged.words.keep)
words.lower <- tolower(words)
word.v <- gsub("[^[:alnum:][:space:]']", "", words.lower)
x <- seq_along(word.v)
if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
} else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]], chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=" ")
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}
topic.m <- NULL
for(i in 1:length(files.v2)){
tagged.text <- scan(file.path(inputDir2, files.v2[i]),
what = "character", sep = "\n")
chunk.m <- makeFlexTextChunksFromTagged(tagged.text,
chunk.size, percentage = FALSE)
textname <- gsub("\\..*", "", files.v2[i])
segments.m <- cbind(paste(textname,
segment=1:nrow(chunk.m), sep = "_"),
chunk.m)
topic.m <- rbind(topic.m, segments.m)
}
documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c("id", "text")
library(mallet)
mallet.instances <- mallet.import(documents$id,
documents$text,
"texts/stoplist.csv",
FALSE,
token.regexp = "[\\p{L}']+")
topic.model <- MalletLDA(num.topics = 5)
class(topic.model)
topic.model$loadDocuments(mallet.instances)
vocab <- topic.model$getVocabulary()
vocab[1:50]
wordfreqs <- mallet.word.freqs(topic.model)
wordfreqs
topic.model$train(400)
topic.words.m <- mallet.topic.words(topic.model,
smoothed = TRUE,
normalized = TRUE)
dim(topic.words.m)
colnames(topic.words.m) <- vocab
library(wordcloud)
for(i in 1:5){
topic.top.words <- mallet.top.words(topic.model,
topic.words.m[i,], 100)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(1.8,.25), rot.per = 0,
random.order = F))
}
rage <- do.call(rbind, lapply(div.freqs.l, '[', 'rage'))
love.rage <- cbind(love, rage)
love <- do.call(rbind, lapply(div.freqs.l, '[', 'love'))
doc <- xmlTreeParse("460-markings-only.xml", useInternalNodes=TRUE)
divs.ns.l <- getNodeSet(doc, "/body//*[@attribution='HM'][not(contains(@mode, 'commentary'))][not(contains(@type, 'notation'))]")
divs.ns.l
divs.checkmark.underline.l <- list()
div.freqs.l <- list()
div.raws.l <- list()
for(i in 1:length(divs.ns.l)){
#specifies div content for retrieval
div.content <- xmlValue(divs.ns.l[[i]], "div")[[1]]
#specifies word tag content for retrieval
words.ns <- xmlElementsByTagName(divs.ns.l[[i]], "w", recursive = TRUE)
#combines word tag content within each div
div.words.v <- paste(sapply(words.ns, xmlValue))
#convert characters to lowercase
words.lower.v <- tolower(div.words.v)
#tokenize words, restricting content to alphabetical characters,
#apostrophes (for contracted word forms), and hyphens (for compound expressions)
words.l <- strsplit(words.lower.v, "[^a-z'-]")
#strsplit product is a list. restore to vector with unlist
word.v <- unlist(words.l)
#remove any remaining whitespace created from culled punctuation
word.v <- word.v[which(word.v!="")]
#create a list of the relevant nodes and their word content
div.freqs.l[[div.content]] <- word.v
#calculate frequencies
div.freqs.t <- table(word.v)
div.raws.l[[div.content]] <- div.freqs.t
div.freqs.l[[div.content]] <- 100*(div.freqs.t/sum(div.freqs.t))
}
divs.checkmark.underline.l
love <- do.call(rbind, lapply(div.freqs.l, '[', 'love'))
rage <- do.call(rbind, lapply(div.freqs.l, '[', 'rage'))
love.rage <- cbind(love, rage)
love.rage[which(is.na(love.rage))] <- 0
colnames(love.rage) <- c("love", "rage")
barplot(love.rage, beside=T, col="grey", main="Frequencies for 'man,' 'world', and 'good' in passages marked by Herman Melville in Shakespeare's Plays", ylab = "Term percentage value within text marked", xlab = "Comedies, Histories, and Tragedies combined")
betrayal <- do.call(rbind, lapply(div.freqs.l, '[', 'betrayal'))
love.betrayal <- cbind(love, betrayal)
love.betrayal[which(is.na(love.betrayal))] <- 0
barplot(love.betrayal, beside=T, col="grey", main="Frequencies for 'man,' 'world', and 'good' in passages marked by Herman Melville in Shakespeare's Plays", ylab = "Term percentage value within text marked", xlab = "Comedies, Histories, and Tragedies combined")
colnames(love.betrayal) <- c("love", "betrayal")
barplot(love.betrayal, beside=T, col="grey", main="Frequencies for 'man,' 'world', and 'good' in passages marked by Herman Melville in Shakespeare's Plays", ylab = "Term percentage value within text marked", xlab = "Comedies, Histories, and Tragedies combined")
